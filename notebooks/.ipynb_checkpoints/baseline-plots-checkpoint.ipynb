{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline-plots\n",
    "\n",
    "## Generating plots with baseline parameters\n",
    "\n",
    "In this Notebook, we generate plots with the baseline paramters used by Maaten in the 2008 t-SNE paper, for the purpose of visual comparison with the optimized plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools & Libraries\n",
    "\n",
    "We use **`Python`**. The following modules are used:\n",
    "\n",
    "* **pandas:** reading, writing and manipulating data.\n",
    "* **numpy:** vectorized calculations and other relevant math functions.\n",
    "* **scipy:** functions for scientific purposes. Great statistics content.\n",
    "* **matplotlib & seaborn:** data visualization.\n",
    "* **sklearn:** comprehensive machine learning libraries.\n",
    "* **hyperopt:** random search and TPE for hyperparameter optimization.\n",
    "* **BayesianOptimization:** Gaussian Processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# opening up a console as the notebook starts\n",
    "%qtconsole\n",
    "\n",
    "# making plots stay on the notebook (no extra windows!)\n",
    "%matplotlib inline\n",
    "\n",
    "# show figures with highest resolution \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# changing working directory\n",
    "import os\n",
    "os.chdir('C:\\\\Users\\\\Guilherme\\\\Documents\\\\TCC\\\\tsne-optim')\n",
    "\n",
    "# importing modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import png, array\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from bayes_opt import BayesianOptimization\n",
    "from hyperopt import fmin, rand, tpe, hp, STATUS_OK, Trials\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline parameters\n",
    "\n",
    "Let us define the baseline set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set of hyperparameters definition - baseline #\n",
    "\n",
    "space_base = {'perplexity': 30,\n",
    "              'early_exaggeration': 4.0,\n",
    "              'learning_rate': 100,\n",
    "              'n_iter': 1000,\n",
    "              'angle': 0.5,\n",
    "              'pca_dims': 30,\n",
    "              'whitening_flag': False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target function\n",
    "\n",
    "Let us replicate the optimization target function to create the plots. We supress the random states definitions to allow for random results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining cost function: KL divergence #\n",
    "\n",
    "# the function takes a search space sample as parameter #\n",
    "def optim_target(data, perplexity, early_exaggeration, learning_rate, n_iter, angle, pca_dims, whitening_flag, n_runs=3):    \n",
    "    \n",
    "    # setting random seed\n",
    "    #np.random.seed(42)\n",
    "    \n",
    "    # store target info\n",
    "    target_var = data['TARGET']; data = data.drop('TARGET', axis=1)\n",
    "    \n",
    "    # HDBSCAN to compute clusters on high dimensional space\n",
    "    clusterer_highd = HDBSCAN(min_cluster_size=10, min_samples=1)\n",
    "\n",
    "    # clustering points\n",
    "    cluster_assign_highd = clusterer_highd.fit_predict(PCA(n_components=100).fit_transform(data))        \n",
    "    \n",
    "    # PCA first to reduce data\n",
    "    reducer = PCA(n_components=pca_dims, whiten=whitening_flag)\n",
    "    \n",
    "    # reducing\n",
    "    reduced_data = reducer.fit_transform(data)\n",
    "    \n",
    "    # let us run t-SNE 5 times and get the best KL div #\n",
    "    \n",
    "    # divergence accumulator, initialize with +infinity\n",
    "    KL_div = np.inf\n",
    "    \n",
    "    # loop for each t-SNE run\n",
    "    for i in range(n_runs):\n",
    "        \n",
    "        # configuring t-SNE\n",
    "        embedder = TSNE(perplexity=perplexity, early_exaggeration=early_exaggeration,\n",
    "                        learning_rate=learning_rate, n_iter=n_iter,\n",
    "                        angle=angle) #random_state=i)\n",
    "        \n",
    "        # fitting\n",
    "        temp_data = embedder.fit_transform(reduced_data)\n",
    "    \n",
    "        # KL divergence result after optimization\n",
    "        temp_div = embedder.kl_divergence_\n",
    "        \n",
    "        # if smaller than last experiment, update\n",
    "        if temp_div < KL_div:\n",
    "            \n",
    "            # updating values\n",
    "            KL_div = temp_div\n",
    "            embedded_data = temp_data\n",
    "    \n",
    "    # data frame form embedded_data\n",
    "    embedded_data = pd.DataFrame({'x': zip(*embedded_data)[0], 'y': zip(*embedded_data)[1]})\n",
    "    \n",
    "    # computing ajusted mutual information over clusterings #\n",
    "\n",
    "    # HDBSCAN to compute clusters on embedded space\n",
    "    clusterer_lowd = HDBSCAN(min_cluster_size=10, min_samples=1)\n",
    "\n",
    "    # clustering points - low-dim\n",
    "    cluster_assign_lowd = clusterer_lowd.fit_predict(embedded_data)\n",
    "\n",
    "    # ajusted mutual info score\n",
    "    AMI_score = adjusted_mutual_info_score(cluster_assign_highd, cluster_assign_lowd)\n",
    "    \n",
    "    # ajusted mutual info on target and dimensions\n",
    "    AMI_target_highd = adjusted_mutual_info_score(target_var, cluster_assign_highd)\n",
    "    AMI_target_lowd = adjusted_mutual_info_score(target_var, cluster_assign_lowd)\n",
    "\n",
    "    # computing global geometry #\n",
    "    \n",
    "    # treating empty centers errors\n",
    "    try:\n",
    "        # centers on high dimensional space\n",
    "        data['assignment'] = cluster_assign_highd; c_groups = data.groupby('assignment').mean()\n",
    "        centers_highd = [np.array(c_groups.iloc[i,:]) for i in c_groups.index if not i == -1]\n",
    "\n",
    "        # distances on high dimensional space\n",
    "        dists_highd = euclidean_distances(centers_highd)\n",
    "        closest_highd = [np.argsort(dists_highd[:,i])[1:] for i in range(dists_highd.shape[0])]\n",
    "        closest_highd_df = pd.DataFrame(np.matrix(closest_highd))\n",
    "\n",
    "        # centers on low dimensional space\n",
    "        embedded_data['assignment'] = cluster_assign_highd; c_groups = embedded_data.groupby('assignment').mean()\n",
    "        centers_lowd = [np.array(c_groups.iloc[i,:]) for i in c_groups.index if not i == -1]\n",
    "\n",
    "        # distances on high dimensional space\n",
    "        dists_lowd = euclidean_distances(centers_lowd)\n",
    "        closest_lowd = [np.argsort(dists_lowd[:,i])[1:] for i in range(dists_lowd.shape[0])]\n",
    "        closest_lowd_df = pd.DataFrame(np.matrix(closest_lowd))\n",
    "\n",
    "        # correlations\n",
    "        rank_order_cor = [spearmanr(closest_lowd_df.iloc[i,:], closest_highd_df.iloc[i,:]).correlation for i in closest_lowd_df.index]\n",
    "        rank_order_cor_score = np.mean(rank_order_cor)\n",
    "    \n",
    "    # lowest value for rank-order corr if error\n",
    "    except ValueError:\n",
    "        rank_order_cor_score = -1.0\n",
    "    \n",
    "    # organizing parameters to return\n",
    "    params = {'perplexity': perplexity,\n",
    "              'early_exaggeration': early_exaggeration,\n",
    "              'learning_rate': learning_rate,\n",
    "              'n_iter': n_iter,\n",
    "              'angle': angle,\n",
    "              'pca_dims': pca_dims,\n",
    "              'whitening_flag': whitening_flag}\n",
    "    \n",
    "    # printing results\n",
    "    print 'KL divergence:', KL_div, '| AMI score:', AMI_score\n",
    "    print 'AMI target-highd:', AMI_target_highd, '| AMI target-lowd:', AMI_target_lowd\n",
    "    print 'Rank-order correlation:', rank_order_cor_score\n",
    "    print 'Parameters:', params\n",
    "    print ' '\n",
    "\n",
    "    # returning values\n",
    "    return KL_div, AMI_score, AMI_target_highd, AMI_target_lowd, rank_order_cor_score, embedded_data, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperopt wrapper\n",
    "\n",
    "Let us use the hyperopt wrapper defined in the other notebooks since it has a nice saving mechanism for the plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wrapper for hyperopt #\n",
    "\n",
    "# hyperopt minimizes functions, so our target value is set\n",
    "class hp_wrapper:\n",
    "    \n",
    "    def __init__(self, data, save_path=None, n_runs=5):\n",
    "        self.data = data\n",
    "        self.save_path = save_path\n",
    "        self.n_runs = n_runs\n",
    "    \n",
    "    def target(self, space):\n",
    "        \n",
    "        # pre-processing space\n",
    "        space['pca_dims'] = int(space['pca_dims'])\n",
    "        space['n_iter'] = int(space['n_iter'])\n",
    "\n",
    "        # running target function\n",
    "        kl_div, ami, ami_th, ami_tl, spearman, embed, params = optim_target(self.data, n_runs=self.n_runs, **space)    \n",
    "        \n",
    "        # if we want to save\n",
    "        if not self.save_path == None:\n",
    "            \n",
    "            # creating path if necessary\n",
    "            if not os.path.exists(self.save_path):\n",
    "                os.makedirs(self.save_path)\n",
    "                \n",
    "            # save name of the plot\n",
    "            save_name = str(max([int(e.split('.')[0]) for e in os.listdir(self.save_path)]+[-1]) + 1) + '.png'\n",
    "            \n",
    "            # title of the plot\n",
    "            plot_title = 'KL divergence: {0:.3f} | AMI score: {1:.3f} | AMI target-highd:'.format(kl_div, ami) + \\\n",
    "                         '{0:.3f} | AMI target-lowd: {1:.3f} | Rank-order correlation: {2:.3f}'.format(ami_th, ami_tl, spearman)\n",
    "    \n",
    "            # subtitle showing parameters\n",
    "            subtitle = '{}'.format(params)\n",
    "\n",
    "            # creating plot\n",
    "            fig = sns.lmplot(x='x', y='y', hue='assignment', data=embed, fit_reg=False, size=9, aspect=1.6); \n",
    "            plt.title(plot_title); plt.xlabel(subtitle)\n",
    "            \n",
    "            # saving\n",
    "            fig.savefig(os.path.join(self.save_path,save_name))\n",
    "            \n",
    "        # a dict with 'loss' and 'status' is required\n",
    "        return {'loss': -ami,\n",
    "                'status': STATUS_OK,\n",
    "                'parameters': params,\n",
    "                'embedding': embed}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving plots for each dataset\n",
    "\n",
    "Let us load each dataset and save 10 plots with the same parameters for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# defining dataset names\n",
    "df_names = ['well_sep', 'well_sep_noise', 'gaussian_noise', 'topology', 'coil_20', 'olivetti']\n",
    "\n",
    "# loop for each dataset\n",
    "for df_name in df_names:\n",
    "    \n",
    "    # path for loading data\n",
    "    path = 'data/final/{}.csv'.format(df_name.replace('_','-'))\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    # creating wrapper object for saving\n",
    "    fig_path = 'vis/baseline_plots/{}'.format(df_name)\n",
    "    task = hp_wrapper(df, save_path=fig_path, n_runs=1)\n",
    "    \n",
    "    # let us generate 5 figures for each dataset\n",
    "    for i in range(5):\n",
    "        \n",
    "        # running t-SNE and saving\n",
    "        task.target(space_base);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
