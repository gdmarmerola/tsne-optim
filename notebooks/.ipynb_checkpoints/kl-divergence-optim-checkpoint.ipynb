{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kl-divergence-optim\n",
    "\n",
    "## Optimization of final Kullback-Leibler divergence\n",
    "\n",
    "In this Notebook, we perform optimization experiments on 6 datasets aiming to set hiperparameters that allow for the lowest KL divergence at the end of t-SNE.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools & Libraries\n",
    "\n",
    "We use **`Python`**. The following modules are used:\n",
    "\n",
    "* **pandas:** reading, writing and manipulating data.\n",
    "* **numpy:** vectorized calculations and other relevant math functions.\n",
    "* **scipy:** functions for scientific purposes. Great statistics content.\n",
    "* **matplotlib & seaborn:** data visualization.\n",
    "* **sklearn:** comprehensive machine learning libraries.\n",
    "* **hyperopt:** random search and TPE for hyperparameter optimization.\n",
    "* **BayesianOptimization:** Gaussian Processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# opening up a console as the notebook starts\n",
    "%qtconsole\n",
    "\n",
    "# making plots stay on the notebook (no extra windows!)\n",
    "%matplotlib inline\n",
    "\n",
    "# show figures with highest resolution \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# changing working directory\n",
    "import os\n",
    "os.chdir('C:\\\\Users\\\\Guilherme\\\\Documents\\\\TCC\\\\tsne-optim')\n",
    "\n",
    "# importing modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import png, array\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from bayes_opt import BayesianOptimization\n",
    "from hyperopt import fmin, rand, tpe, hp, STATUS_OK, Trials\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Search spaces\n",
    "\n",
    "Let us define our hyperparameter search space, both for **`hyperopt`** and **`BayesianOptimization`**. Furthermore, we also define a **baseline** set of parameters, used by Maaten et. al. in the [2008 paper](http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) that first proposed t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# choice of perplexity, PCA dims and whitening\n",
    "perp_default = 30; pca_dims_default = 30; whitening_default = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# search space definition - hyperopt #\n",
    "\n",
    "space_hp = {'perplexity': hp.choice('perp',[perp_default]), # as we are optimizing KL divergence, perplexity stays fixed\n",
    "            'early_exaggeration': hp.uniform('exag', 1.0, 12.0),\n",
    "            'learning_rate': hp.uniform('lr', 50, 1000),\n",
    "            'n_iter': hp.quniform('ni', 200, 5000, 100),\n",
    "            'angle': hp.uniform('angle', 0.2, 0.8),\n",
    "            'pca_dims': hp.choice('pca', [pca_dims_default]) # as we are optimizing KL divergence, PCA dims stays fixed\n",
    "            'whitening_flag': hp.choice('white', [whitening_default])} # as we are optimizing KL divergence, whitening stays fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# search space definition - bayesian optimzation #\n",
    "\n",
    "space_bo = {'perplexity': (perp_default,perp_default), # as we are optimizing KL divergence, perplexity stays fixed\n",
    "            'early_exaggeration': (1.0, 12.0),\n",
    "            'learning_rate': (50, 1000),\n",
    "            'n_iter': (200, 5000),\n",
    "            'angle': (0.2,0.8),\n",
    "            'pca_dims': (pca_dims_default,pca_dims_default),\n",
    "            'whitening_flag': (int(whitening_default), int(whitening_default))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set of hyperparameters definition - baseline #\n",
    "\n",
    "space_base = {'perplexity': 30,\n",
    "              'early_exaggeration': 4.0,\n",
    "              'learning_rate': 100,\n",
    "              'n_iter': 1000,\n",
    "              'angle': 0.5,\n",
    "              'pca_dims': 30,\n",
    "              'whitening_flag': False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target function\n",
    "\n",
    "Let us define our optimization objective. Here we run t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# defining cost function: KL divergence #\n",
    "\n",
    "# the function takes a search space sample as parameter #\n",
    "def optim_target(data, perplexity, early_exaggeration, learning_rate, n_iter, angle, pca_dims, whitening_flag, n_runs=3):    \n",
    "    \n",
    "    # setting random seed\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # store target info\n",
    "    target_var = data['TARGET']; data = data.drop('TARGET', axis=1)\n",
    "    \n",
    "    # HDBSCAN to compute clusters on high dimensional space\n",
    "    # HDBSCAN(min_cluster_size=10, min_samples=1, allow_single_cluster=True)\n",
    "    clusterer_highd = HDBSCAN(min_cluster_size=10, min_samples=1, allow_single_cluster=True)\n",
    "\n",
    "    # clustering points\n",
    "    cluster_assign_highd = clusterer_highd.fit_predict(PCA(n_components=100).fit_transform(data))        \n",
    "    \n",
    "    # PCA first to reduce data\n",
    "    reducer = PCA(n_components=pca_dims, whiten=whitening_flag)\n",
    "    \n",
    "    # reducing\n",
    "    reduced_data = reducer.fit_transform(data)\n",
    "    \n",
    "    # let us run t-SNE 5 times and get the best KL div #\n",
    "    \n",
    "    # divergence accumulator, initialize with +infinity\n",
    "    KL_div = np.inf\n",
    "    \n",
    "    # loop for each t-SNE run\n",
    "    for i in range(n_runs):\n",
    "        \n",
    "        # configuring t-SNE\n",
    "        embedder = TSNE(perplexity=perplexity, early_exaggeration=early_exaggeration,\n",
    "                        learning_rate=learning_rate, n_iter=n_iter,\n",
    "                        angle=angle, random_state=i)\n",
    "        \n",
    "        # fitting\n",
    "        temp_data = embedder.fit_transform(reduced_data)\n",
    "    \n",
    "        # KL divergence result after optimization\n",
    "        temp_div = embedder.kl_divergence_\n",
    "        \n",
    "        # if smaller than last experiment, update\n",
    "        if temp_div < KL_div:\n",
    "            \n",
    "            # updating values\n",
    "            KL_div = temp_div\n",
    "            embedded_data = temp_data\n",
    "    \n",
    "    # data frame form embedded_data\n",
    "    embedded_data = pd.DataFrame({'x': zip(*embedded_data)[0], 'y': zip(*embedded_data)[1]})\n",
    "    \n",
    "    # computing ajusted mutual information over clusterings #\n",
    "\n",
    "    # HDBSCAN to compute clusters on embedded space\n",
    "    clusterer_lowd = HDBSCAN(min_cluster_size=10, min_samples=1, allow_single_cluster=True)\n",
    "\n",
    "    # clustering points - low-dim\n",
    "    cluster_assign_lowd = clusterer_lowd.fit_predict(embedded_data)\n",
    "\n",
    "    # ajusted mutual info score\n",
    "    AMI_score = adjusted_mutual_info_score(cluster_assign_highd, cluster_assign_lowd)\n",
    "    \n",
    "    # ajusted mutual info on target and dimensions\n",
    "    AMI_target_highd = adjusted_mutual_info_score(target_var, cluster_assign_highd)\n",
    "    AMI_target_lowd = adjusted_mutual_info_score(target_var, cluster_assign_lowd)\n",
    "\n",
    "    # computing global geometry #\n",
    "    \n",
    "    # treating empty centers errors\n",
    "    try:\n",
    "        # centers on high dimensional space\n",
    "        data['assignment'] = cluster_assign_highd; c_groups = data.groupby('assignment').mean()\n",
    "        centers_highd = [np.array(c_groups.iloc[i,:]) for i in c_groups.index if not i == -1]\n",
    "\n",
    "        # distances on high dimensional space\n",
    "        dists_highd = euclidean_distances(centers_highd)\n",
    "        closest_highd = [np.argsort(dists_highd[:,i])[1:] for i in range(dists_highd.shape[0])]\n",
    "        closest_highd_df = pd.DataFrame(np.matrix(closest_highd))\n",
    "\n",
    "        # centers on low dimensional space\n",
    "        embedded_data['assignment'] = cluster_assign_highd; c_groups = embedded_data.groupby('assignment').mean()\n",
    "        centers_lowd = [np.array(c_groups.iloc[i,:]) for i in c_groups.index if not i == -1]\n",
    "\n",
    "        # distances on high dimensional space\n",
    "        dists_lowd = euclidean_distances(centers_lowd)\n",
    "        closest_lowd = [np.argsort(dists_lowd[:,i])[1:] for i in range(dists_lowd.shape[0])]\n",
    "        closest_lowd_df = pd.DataFrame(np.matrix(closest_lowd))\n",
    "\n",
    "        # correlations\n",
    "        rank_order_cor = [spearmanr(closest_lowd_df.iloc[i,:], closest_highd_df.iloc[i,:]).correlation for i in closest_lowd_df.index]\n",
    "        rank_order_cor_score = np.mean(rank_order_cor)\n",
    "        \n",
    "        # if missing value, make it -1.0\n",
    "        if pd.isnull(rank_order_cor_score):\n",
    "            rank_order_cor_score = -1.0\n",
    "        \n",
    "    # lowest value for rank-order corr if error\n",
    "    except ValueError:\n",
    "        rank_order_cor_score = -1.0\n",
    "    \n",
    "    # organizing parameters to return\n",
    "    params = {'perplexity': perplexity,\n",
    "              'early_exaggeration': early_exaggeration,\n",
    "              'learning_rate': learning_rate,\n",
    "              'n_iter': n_iter,\n",
    "              'angle': angle,\n",
    "              'pca_dims': pca_dims,\n",
    "              'whitening_flag': whitening_flag}\n",
    "    \n",
    "    # adding target variable to embedded data\n",
    "    embedded_data.loc[:,'target'] = target_var\n",
    "    \n",
    "    # printing results\n",
    "    print 'KL divergence:', KL_div, '| AMI score:', AMI_score\n",
    "    print 'AMI target-highd:', AMI_target_highd, '| AMI target-lowd:', AMI_target_lowd\n",
    "    print 'Rank-order correlation:', rank_order_cor_score\n",
    "    print 'Parameters:', params\n",
    "    print ' '\n",
    "\n",
    "    # returning values\n",
    "    return KL_div, AMI_score, AMI_target_highd, AMI_target_lowd, rank_order_cor_score, embedded_data, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# testing the function #\n",
    "\n",
    "# let us use coil-20 dataset on baseline parameters\n",
    "test_df = pd.read_csv('data/final/coil-20.csv')\n",
    "\n",
    "# removing target variable\n",
    "test_target = test_df['TARGET']\n",
    "\n",
    "# running target function\n",
    "kl_div, ami, ami_th, ami_tl, spearman, embed, params = optim_target(test_df, n_runs=1, **space_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let us study the result #\n",
    "sns.lmplot(x='x', y='y', hue='target', data=embed, fit_reg=False, size=6, aspect=1.6); plt.title('Test of target function');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Wrappers\n",
    "\n",
    "We need to create wrappers for the target function, so it can work with the optimization packages we chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wrapper for hyperopt #\n",
    "\n",
    "# hyperopt minimizes functions, so our target value is set\n",
    "class hp_wrapper:\n",
    "    \n",
    "    def __init__(self, data, save_path=None, n_runs=5):\n",
    "        self.data = data\n",
    "        self.save_path = save_path\n",
    "        self.n_runs = n_runs\n",
    "    \n",
    "    def target(self, space):\n",
    "        \n",
    "        # pre-processing space\n",
    "        space['pca_dims'] = int(space['pca_dims'])\n",
    "        space['n_iter'] = int(space['n_iter'])\n",
    "        \n",
    "        # trying to ignore errors\n",
    "        try:        \n",
    "            # running target function\n",
    "            kl_div, ami, ami_th, ami_tl, spearman, embed, params = optim_target(self.data, n_runs=self.n_runs, **space)    \n",
    "\n",
    "            # if we want to save\n",
    "            if not self.save_path == None:\n",
    "\n",
    "                # creating path if necessary\n",
    "                if not os.path.exists(self.save_path):\n",
    "                    os.makedirs(self.save_path)\n",
    "\n",
    "                # save name of the plot\n",
    "                save_name = str(max([int(e.split('.')[0]) for e in os.listdir(self.save_path)]+[-1]) + 1) + '.png'\n",
    "\n",
    "                # title of the plot\n",
    "                plot_title = 'KL divergence: {0:.3f} | AMI score: {1:.3f} | AMI target-highd:'.format(kl_div, ami) + \\\n",
    "                             '{0:.3f} | AMI target-lowd: {1:.3f} | Rank-order correlation: {2:.3f}'.format(ami_th, ami_tl, spearman)\n",
    "\n",
    "                # subtitle showing parameters\n",
    "                subtitle = '{}'.format(params)\n",
    "\n",
    "                # creating plot\n",
    "                fig = sns.lmplot(x='x', y='y', hue='target', data=embed, fit_reg=False, size=9, aspect=1.6, legend=False); \n",
    "                plt.title(plot_title); plt.xlabel(subtitle)\n",
    "\n",
    "                # saving\n",
    "                fig.savefig(os.path.join(self.save_path,save_name))\n",
    "                \n",
    "                # do not show plot on jupyter\n",
    "                plt.close()\n",
    "\n",
    "            # a dict with 'loss' and 'status' is required\n",
    "            return {'loss': kl_div,\n",
    "                    'status': STATUS_OK,\n",
    "                    'parameters': params,\n",
    "                    'embedding': embed}\n",
    "        \n",
    "        # catching exception\n",
    "        except Exception as e:\n",
    "            print 'An Error occurred:', e\n",
    "            \n",
    "            # a dict with 'loss' and 'status' is required\n",
    "            return {'loss': 1e9,\n",
    "                    'status': 'fail',\n",
    "                    'parameters': space,\n",
    "                    'embedding': 'error'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wrapper for BayesianOptimization #\n",
    "\n",
    "# we need to coerce some variables to int, as the gaussian process regression only works with floats\n",
    "class bo_wrapper:\n",
    "    \n",
    "    def __init__(self, data, save_path=None, n_runs=5):\n",
    "        self.data = data\n",
    "        self.save_path = save_path\n",
    "        self.n_runs = n_runs\n",
    "        \n",
    "    def target(self, perplexity, early_exaggeration, learning_rate, n_iter, angle, pca_dims, whitening_flag):\n",
    "    \n",
    "        # pre-processing space\n",
    "        pca_dims = int(pca_dims)\n",
    "        n_iter = int(n_iter)\n",
    "\n",
    "        # proxy for whitening flag\n",
    "        if whitening_flag < 0.50:\n",
    "            whitening_flag = False\n",
    "        else:\n",
    "            whitening_flag = True\n",
    "        \n",
    "        # hadling errors\n",
    "        try:\n",
    "    \n",
    "            # running target function\n",
    "            kl_div, ami, ami_th, ami_tl, spearman, embed, params = optim_target(self.data, perplexity, early_exaggeration, \n",
    "                                                                                learning_rate, n_iter, angle, pca_dims, \n",
    "                                                                                whitening_flag, n_runs=self.n_runs)\n",
    "            # if we want to save\n",
    "            if not self.save_path == None:\n",
    "\n",
    "                # creating path if necessary\n",
    "                if not os.path.exists(self.save_path):\n",
    "                    os.makedirs(self.save_path)\n",
    "\n",
    "                # save name of the plot\n",
    "                save_name = str(max([int(e.split('.')[0]) for e in os.listdir(self.save_path)]+[-1]) + 1) + '.png'\n",
    "\n",
    "                # title of the plot\n",
    "                plot_title = 'KL divergence: {0:.3f} | AMI score: {1:.3f} | AMI target-highd:'.format(kl_div, ami) + \\\n",
    "                             '{0:.3f} | AMI target-lowd: {1:.3f} | Rank-order correlation: {2:.3f}'.format(ami_th, ami_tl, spearman)\n",
    "\n",
    "                # subtitle showing parameters\n",
    "                subtitle = '{}'.format(params)\n",
    "\n",
    "                # creating plot\n",
    "                fig = sns.lmplot(x='x', y='y', hue='target', data=embed, fit_reg=False, size=9, aspect=1.6, legend=False); \n",
    "                plt.title(plot_title); plt.xlabel(subtitle)\n",
    "\n",
    "                # saving\n",
    "                fig.savefig(os.path.join(self.save_path,save_name))\n",
    "                \n",
    "                # do not show plot on jupyter\n",
    "                plt.close()\n",
    "\n",
    "            # retuning target value: negative because BO maximizes functions\n",
    "            return -kl_div\n",
    "        \n",
    "        # catching exception\n",
    "        except Exception as e:\n",
    "            print 'An Error occurred:', e\n",
    "            \n",
    "            # a dict with 'loss' and 'status' is required\n",
    "            return 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testing hyperopt wrapper\n",
    "opt_task = hp_wrapper(test_df, save_path='vis/kl_div_optim2/all_plots/test_figs', n_runs=1)\n",
    "opt_task.target(space_base)['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testing bayes_opt wrapper\n",
    "opt_task = bo_wrapper(test_df, save_path='vis/kl_div_optim2/all_plots/test_figs', n_runs=1)\n",
    "opt_task.target(**space_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimization Tasks\n",
    "\n",
    "Let us now create and execute our optimization tasks. We need to run 3 optimization procedures: Random Search, TPE and Gaussian Processes for 6 datasets. Let us divide our analysis by dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let us define an experiment dict #\n",
    "# and index the results by dataset and optimization method #\n",
    "# e.g. exp_dict['well_sep']['random_search'] refers to the results #\n",
    "# of random search no the well-separated clusters data #\n",
    "\n",
    "# defining dict\n",
    "experiment_dict = dict()\n",
    "\n",
    "# initializing dict\n",
    "for dataset in ['well_sep', 'well_sep_noise', 'gaussian_noise', 'topology', 'coil_20', 'olivetti']:\n",
    "    for optim in ['rand', 'tpe', 'gp']:\n",
    "        try:\n",
    "            experiment_dict[dataset][optim] = dict()\n",
    "        except KeyError:\n",
    "            experiment_dict[dataset] = dict()\n",
    "            experiment_dict[dataset][optim] = dict()\n",
    "            \n",
    "# checking the result\n",
    "experiment_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Well-separated clusters\n",
    "\n",
    "Set of 8 well-separated gaussian blobs in a 100-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading the data\n",
    "test_df = pd.read_csv('data/final/well-sep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# random search #\n",
    "\n",
    "# initializing wrapper - hyperopt\n",
    "opt_task_hp = hp_wrapper(test_df, save_path='vis/kl_div_optim2/all_plots/well_sep_rand')\n",
    "\n",
    "# trials object stores the evaluations\n",
    "trials_rand = Trials()\n",
    "\n",
    "# using the fmin function from hyperopt\n",
    "best = fmin(fn=opt_task_hp.target, algo=rand.suggest, space=space_hp, max_evals=50, trials=trials_rand)\n",
    "\n",
    "# storing the results\n",
    "experiment_dict['well_sep']['rand'] = [trials_rand.trials[i]['result'] for i in range(len(trials_rand.trials))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TPE #\n",
    "\n",
    "# initializing wrapper - hyperopt\n",
    "opt_task_hp = hp_wrapper(test_df, save_path='vis/kl_div_optim2/all_plots/well_sep_tpe')\n",
    "\n",
    "# trials object stores the evaluations\n",
    "trials_tpe = Trials()\n",
    "\n",
    "# using the fmin function from hyperopt\n",
    "best = fmin(fn=opt_task_hp.target, algo=tpe.suggest, space=space_hp, max_evals=50, trials=trials_tpe)\n",
    "\n",
    "# storing the results\n",
    "experiment_dict['well_sep']['tpe'] = [trials_tpe.trials[i]['result'] for i in range(len(trials_tpe.trials))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gaussian processes #\n",
    "\n",
    "# initializing wrapper - BayesianOptimazion\n",
    "opt_task_bo = bo_wrapper(test_df, save_path='vis/kl_div_optim2/all_plots/well_sep_gp')\n",
    "\n",
    "# defining optimization object\n",
    "bo = BayesianOptimization(opt_task_bo.target, space_bo, verbose=0)\n",
    "\n",
    "# optimizing\n",
    "bo.maximize(init_points=10, n_iter=40, acq='ucb', kappa=10)\n",
    "\n",
    "# writing results\n",
    "experiment_dict['well_sep']['gp'] = bo.res['all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# computing figures of the optimization process #\n",
    "\n",
    "# function to transform the experiment dict into a data frame - hyperopt #\n",
    "def exp_dict_to_df_hp(exp_dict):\n",
    "\n",
    "    # dictionary to story intermediate values\n",
    "    temp_dict = dict()\n",
    "\n",
    "    # for each experiment\n",
    "    for dict_entry in exp_dict:\n",
    "\n",
    "        # add parameters to dict\n",
    "        for key in dict_entry['parameters'].keys():\n",
    "            try:\n",
    "                temp_dict[key].append(dict_entry['parameters'][key])\n",
    "            except KeyError:\n",
    "                temp_dict[key] = [dict_entry['parameters'][key]]\n",
    "\n",
    "        # add loss to dict\n",
    "        try:\n",
    "            temp_dict['loss'].append(dict_entry['loss'])\n",
    "        except KeyError:\n",
    "            temp_dict['loss'] = [dict_entry['loss']]\n",
    "    \n",
    "    # return a data frame\n",
    "    return pd.DataFrame(temp_dict)\n",
    "\n",
    "# function to transform the experiment dict into a data frame - bayesian optim #\n",
    "def exp_dict_to_df_bo(exp_dict):\n",
    "\n",
    "    # dictionary to story intermediate values\n",
    "    temp_dict = dict()\n",
    "\n",
    "    # for each experiment\n",
    "    for dict_entry in exp_dict['params']:\n",
    "\n",
    "        # add parameters to dict\n",
    "        for key in dict_entry.keys():\n",
    "            try:\n",
    "                temp_dict[key].append(dict_entry[key])\n",
    "            except KeyError:\n",
    "                temp_dict[key] = [dict_entry[key]]\n",
    "    \n",
    "    # adding loss\n",
    "    temp_dict['loss'] = exp_dict['values']\n",
    "    \n",
    "    # correcting whitening_flag\n",
    "    temp_dict['whitening_flag'] = [False if e < 0.50 else True for e in temp_dict['whitening_flag']]\n",
    "    \n",
    "    # return a data frame\n",
    "    return pd.DataFrame(temp_dict)\n",
    "\n",
    "# random search #\n",
    "\n",
    "# results data frame\n",
    "rand_res_df = exp_dict_to_df_hp(experiment_dict['well_sep']['rand'])\n",
    "\n",
    "# plotting hyperparameter importance\n",
    "g = sns.PairGrid(rand_res_df, hue='loss');\n",
    "g.map_diag(plt.hist);\n",
    "g.map_offdiag(plt.scatter);\n",
    "g.savefig('vis/kl_div_optim2/hyperparam_maps/well_sep_rand.pdf');\n",
    "\n",
    "# plotting loss function evolution\n",
    "fig = plt.figure(figsize=[16,9]); plt.step(range(len(rand_res_df['loss'])), rand_res_df['loss'], where='mid'); \n",
    "plt.title('Loss over rounds (Random Search)'); plt.xlabel('Round'); plt.ylabel('Loss'); \n",
    "fig.savefig('vis/kl_div_optim2/loss_over_rounds/well_sep_rand.pdf');\n",
    "\n",
    "# tpe #\n",
    "\n",
    "# results data frame\n",
    "tpe_res_df = exp_dict_to_df_hp(experiment_dict['well_sep']['tpe'])\n",
    "\n",
    "# plotting hyperparameter importance\n",
    "g = sns.PairGrid(tpe_res_df, hue='loss');\n",
    "g.map_diag(plt.hist);\n",
    "g.map_offdiag(plt.scatter);\n",
    "g.savefig('vis/kl_div_optim2/hyperparam_maps/well_sep_tpe.pdf');\n",
    "\n",
    "# plotting loss function evolution\n",
    "fig = plt.figure(figsize=[16,9]); plt.step(range(len(tpe_res_df['loss'])), tpe_res_df['loss'], where='mid'); \n",
    "plt.title('Loss over rounds (TPE)'); plt.xlabel('Round'); plt.ylabel('Loss'); \n",
    "fig.savefig('vis/kl_div_optim2/loss_over_rounds/well_sep_tpe.pdf');\n",
    "\n",
    "# gaussian processes #\n",
    "\n",
    "# results data frame\n",
    "gp_res_df = exp_dict_to_df_bo(experiment_dict['well_sep']['gp'])\n",
    "\n",
    "# plotting hyperparameter importance\n",
    "g = sns.PairGrid(gp_res_df, hue='loss');\n",
    "g.map_diag(plt.hist);\n",
    "g.map_offdiag(plt.scatter);\n",
    "g.savefig('vis/kl_div_optim2/hyperparam_maps/well_sep_gp.pdf');\n",
    "\n",
    "# plotting loss function evolution\n",
    "fig = plt.figure(figsize=[16,9]); plt.step(range(len(gp_res_df['loss'])), -gp_res_df['loss'], where='mid'); \n",
    "plt.title('Loss over rounds (GP)'); plt.xlabel('Round'); plt.ylabel('Loss'); \n",
    "fig.savefig('vis/kl_div_optim2/loss_over_rounds/well_sep_gp.pdf');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# showing best embedded spaces #\n",
    "print \"Random Search best:\", [i for i,e in enumerate(experiment_dict['well_sep']['rand']) if e['loss'] == np.min([e['loss'] for e in experiment_dict['well_sep']['rand']])]\n",
    "print \"TPE best:\", [i for i,e in enumerate(experiment_dict['well_sep']['tpe']) if e['loss'] == np.min([e['loss'] for e in experiment_dict['well_sep']['tpe']])]\n",
    "print \"GP best:\", [i for i,e in enumerate(experiment_dict['well_sep']['gp']['values']) if e == np.max(experiment_dict['well_sep']['gp']['values'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Well-separated clusters with noise\n",
    "\n",
    "Set of 8 well-separated gaussian blobs in a 100-dimensional space under uniform noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading the data\n",
    "test_df = pd.read_csv('data/final/well-sep-noise.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# removing target variable\n",
    "test_target = test_df['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# random search #\n",
    "\n",
    "# initializing wrapper - hyperopt\n",
    "opt_task_hp = hp_wrapper(test_df, save_path='vis/kl_div_optim2/all_plots/well_sep_noise_rand')\n",
    "\n",
    "# trials object stores the evaluations\n",
    "trials_rand = Trials()\n",
    "\n",
    "# using the fmin function from hyperopt\n",
    "best = fmin(fn=opt_task_hp.target, algo=rand.suggest, space=space_hp, max_evals=50, trials=trials_rand)\n",
    "\n",
    "# storing the results\n",
    "experiment_dict['well_sep_noise']['rand'] = [trials_rand.trials[i]['result'] for i in range(len(trials_rand.trials))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TPE #\n",
    "\n",
    "# initializing wrapper - hyperopt\n",
    "opt_task_hp = hp_wrapper(test_df, save_path='vis/kl_div_optim2/all_plots/well_sep_noise_tpe')\n",
    "\n",
    "# trials object stores the evaluations\n",
    "trials_tpe = Trials()\n",
    "\n",
    "# using the fmin function from hyperopt\n",
    "best = fmin(fn=opt_task_hp.target, algo=tpe.suggest, space=space_hp, max_evals=50, trials=trials_tpe)\n",
    "\n",
    "# storing the results\n",
    "experiment_dict['well_sep_noise']['tpe'] = [trials_tpe.trials[i]['result'] for i in range(len(trials_tpe.trials))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gaussian processes #\n",
    "\n",
    "# initializing wrapper - BayesianOptimazion\n",
    "opt_task_bo = bo_wrapper(test_df, save_path='vis/kl_div_optim2/all_plots/well_sep_noise_gp')\n",
    "\n",
    "# defining optimization object\n",
    "bo = BayesianOptimization(opt_task_bo.target, space_bo, verbose=0)\n",
    "\n",
    "# optimizing\n",
    "bo.maximize(init_points=10, n_iter=40, acq='ucb', kappa=10)\n",
    "\n",
    "# writing results\n",
    "experiment_dict['well_sep_noise']['gp'] = bo.res['all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# computing figures of the optimization process #\n",
    "\n",
    "# function to transform the experiment dict into a data frame - hyperopt #\n",
    "def exp_dict_to_df_hp(exp_dict):\n",
    "\n",
    "    # dictionary to story intermediate values\n",
    "    temp_dict = dict()\n",
    "\n",
    "    # for each experiment\n",
    "    for dict_entry in exp_dict:\n",
    "\n",
    "        # add parameters to dict\n",
    "        for key in dict_entry['parameters'].keys():\n",
    "            try:\n",
    "                temp_dict[key].append(dict_entry['parameters'][key])\n",
    "            except KeyError:\n",
    "                temp_dict[key] = [dict_entry['parameters'][key]]\n",
    "\n",
    "        # add loss to dict\n",
    "        try:\n",
    "            temp_dict['loss'].append(dict_entry['loss'])\n",
    "        except KeyError:\n",
    "            temp_dict['loss'] = [dict_entry['loss']]\n",
    "    \n",
    "    # return a data frame\n",
    "    return pd.DataFrame(temp_dict)\n",
    "\n",
    "# function to transform the experiment dict into a data frame - bayesian optim #\n",
    "def exp_dict_to_df_bo(exp_dict):\n",
    "\n",
    "    # dictionary to story intermediate values\n",
    "    temp_dict = dict()\n",
    "\n",
    "    # for each experiment\n",
    "    for dict_entry in exp_dict['params']:\n",
    "\n",
    "        # add parameters to dict\n",
    "        for key in dict_entry.keys():\n",
    "            try:\n",
    "                temp_dict[key].append(dict_entry[key])\n",
    "            except KeyError:\n",
    "                temp_dict[key] = [dict_entry[key]]\n",
    "    \n",
    "    # adding loss\n",
    "    temp_dict['loss'] = exp_dict['values']\n",
    "    \n",
    "    # correcting whitening_flag\n",
    "    temp_dict['whitening_flag'] = [False if e < 0.50 else True for e in temp_dict['whitening_flag']]\n",
    "    \n",
    "    # return a data frame\n",
    "    return pd.DataFrame(temp_dict)\n",
    "\n",
    "# random search #\n",
    "\n",
    "# results data frame\n",
    "rand_res_df = exp_dict_to_df_hp(experiment_dict['well_sep_noise']['rand'])\n",
    "\n",
    "# plotting hyperparameter importance\n",
    "g = sns.PairGrid(rand_res_df, hue='loss');\n",
    "g.map_diag(plt.hist);\n",
    "g.map_offdiag(plt.scatter);\n",
    "g.savefig('vis/kl_div_optim2/hyperparam_maps/well_sep_noise_rand.pdf');\n",
    "\n",
    "# plotting loss function evolution\n",
    "fig = plt.figure(figsize=[16,9]); plt.step(range(len(rand_res_df['loss'])), rand_res_df['loss'], where='mid'); \n",
    "plt.title('Loss over rounds (Random Search)'); plt.xlabel('Round'); plt.ylabel('Loss'); \n",
    "fig.savefig('vis/kl_div_optim2/loss_over_rounds/well_sep_noise_rand.pdf');\n",
    "\n",
    "# tpe #\n",
    "\n",
    "# results data frame\n",
    "tpe_res_df = exp_dict_to_df_hp(experiment_dict['well_sep_noise']['tpe'])\n",
    "\n",
    "# plotting hyperparameter importance\n",
    "g = sns.PairGrid(tpe_res_df, hue='loss');\n",
    "g.map_diag(plt.hist);\n",
    "g.map_offdiag(plt.scatter);\n",
    "g.savefig('vis/kl_div_optim2/hyperparam_maps/well_sep_noise_tpe.pdf');\n",
    "\n",
    "# plotting loss function evolution\n",
    "fig = plt.figure(figsize=[16,9]); plt.step(range(len(tpe_res_df['loss'])), tpe_res_df['loss'], where='mid'); \n",
    "plt.title('Loss over rounds (TPE)'); plt.xlabel('Round'); plt.ylabel('Loss'); \n",
    "fig.savefig('vis/kl_div_optim2/loss_over_rounds/well_sep_noise_tpe.pdf');\n",
    "\n",
    "# gaussian processes #\n",
    "\n",
    "# results data frame\n",
    "gp_res_df = exp_dict_to_df_bo(experiment_dict['well_sep_noise']['gp'])\n",
    "\n",
    "# plotting hyperparameter importance\n",
    "g = sns.PairGrid(gp_res_df, hue='loss');\n",
    "g.map_diag(plt.hist);\n",
    "g.map_offdiag(plt.scatter);\n",
    "g.savefig('vis/kl_div_optim2/hyperparam_maps/well_sep_noise_gp.pdf');\n",
    "\n",
    "# plotting loss function evolution\n",
    "fig = plt.figure(figsize=[16,9]); plt.step(range(len(gp_res_df['loss'])), -gp_res_df['loss'], where='mid'); \n",
    "plt.title('Loss over rounds (GP)'); plt.xlabel('Round'); plt.ylabel('Loss'); \n",
    "fig.savefig('vis/kl_div_optim2/loss_over_rounds/well_sep_noise_gp.pdf');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# showing best embedded spaces #\n",
    "print \"Random Search best:\", [i for i,e in enumerate(experiment_dict['well_sep_noise']['rand']) if e['loss'] == np.min([e['loss'] for e in experiment_dict['well_sep_noise']['rand']])]\n",
    "print \"TPE best:\", [i for i,e in enumerate(experiment_dict['well_sep_noise']['tpe']) if e['loss'] == np.min([e['loss'] for e in experiment_dict['well_sep_noise']['tpe']])]\n",
    "print \"GP best:\", [i for i,e in enumerate(experiment_dict['well_sep_noise']['gp']['values']) if e == np.max(experiment_dict['well_sep_noise']['gp']['values'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 200-dimensional gaussian noise\n",
    "\n",
    "Gaussian noise centered on origin. No clusters nor significant patterns to show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading the data\n",
    "test_df = pd.read_csv('data/final/gaussian-noise.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# removing target variable\n",
    "test_target = test_df['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# random search #\n",
    "\n",
    "# initializing wrapper - hyperopt\n",
    "opt_task_hp = hp_wrapper(test_df, save_path='vis/kl_div_optim2/all_plots/gaussian_noise_rand')\n",
    "\n",
    "# trials object stores the evaluations\n",
    "trials_rand = Trials()\n",
    "\n",
    "# using the fmin function from hyperopt\n",
    "best = fmin(fn=opt_task_hp.target, algo=rand.suggest, space=space_hp, max_evals=50, trials=trials_rand)\n",
    "\n",
    "# storing the results\n",
    "experiment_dict['gaussian_noise']['rand'] = [trials_rand.trials[i]['result'] for i in range(len(trials_rand.trials))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TPE #\n",
    "\n",
    "# initializing wrapper - hyperopt\n",
    "opt_task_hp = hp_wrapper(test_df, save_path='vis/kl_div_optim2/all_plots/gaussian_noise_tpe')\n",
    "\n",
    "# trials object stores the evaluations\n",
    "trials_tpe = Trials()\n",
    "\n",
    "# using the fmin function from hyperopt\n",
    "best = fmin(fn=opt_task_hp.target, algo=tpe.suggest, space=space_hp, max_evals=50, trials=trials_tpe)\n",
    "\n",
    "# storing the results\n",
    "experiment_dict['gaussian_noise']['tpe'] = [trials_tpe.trials[i]['result'] for i in range(len(trials_tpe.trials))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gaussian processes #\n",
    "\n",
    "# initializing wrapper - BayesianOptimazion\n",
    "opt_task_bo = bo_wrapper(test_df, save_path='vis/kl_div_optim2/all_plots/gaussian_noise_gp')\n",
    "\n",
    "# defining optimization object\n",
    "bo = BayesianOptimization(opt_task_bo.target, space_bo, verbose=0)\n",
    "\n",
    "# optimizing\n",
    "bo.maximize(init_points=10, n_iter=40, acq='ucb', kappa=10)\n",
    "\n",
    "# writing results\n",
    "experiment_dict['gaussian_noise']['gp'] = bo.res['all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# computing figures of the optimization process #\n",
    "\n",
    "# function to transform the experiment dict into a data frame - hyperopt #\n",
    "def exp_dict_to_df_hp(exp_dict):\n",
    "\n",
    "    # dictionary to story intermediate values\n",
    "    temp_dict = dict()\n",
    "\n",
    "    # for each experiment\n",
    "    for dict_entry in exp_dict:\n",
    "\n",
    "        # add parameters to dict\n",
    "        for key in dict_entry['parameters'].keys():\n",
    "            try:\n",
    "                temp_dict[key].append(dict_entry['parameters'][key])\n",
    "            except KeyError:\n",
    "                temp_dict[key] = [dict_entry['parameters'][key]]\n",
    "\n",
    "        # add loss to dict\n",
    "        try:\n",
    "            temp_dict['loss'].append(dict_entry['loss'])\n",
    "        except KeyError:\n",
    "            temp_dict['loss'] = [dict_entry['loss']]\n",
    "    \n",
    "    # return a data frame\n",
    "    return pd.DataFrame(temp_dict)\n",
    "\n",
    "# function to transform the experiment dict into a data frame - bayesian optim #\n",
    "def exp_dict_to_df_bo(exp_dict):\n",
    "\n",
    "    # dictionary to story intermediate values\n",
    "    temp_dict = dict()\n",
    "\n",
    "    # for each experiment\n",
    "    for dict_entry in exp_dict['params']:\n",
    "\n",
    "        # add parameters to dict\n",
    "        for key in dict_entry.keys():\n",
    "            try:\n",
    "                temp_dict[key].append(dict_entry[key])\n",
    "            except KeyError:\n",
    "                temp_dict[key] = [dict_entry[key]]\n",
    "    \n",
    "    # adding loss\n",
    "    temp_dict['loss'] = exp_dict['values']\n",
    "    \n",
    "    # correcting whitening_flag\n",
    "    temp_dict['whitening_flag'] = [False if e < 0.50 else True for e in temp_dict['whitening_flag']]\n",
    "    \n",
    "    # return a data frame\n",
    "    return pd.DataFrame(temp_dict)\n",
    "\n",
    "# random search #\n",
    "\n",
    "# results data frame\n",
    "rand_res_df = exp_dict_to_df_hp(experiment_dict['gaussian_noise']['rand'])\n",
    "\n",
    "# plotting hyperparameter importance\n",
    "g = sns.PairGrid(rand_res_df, hue='loss');\n",
    "g.map_diag(plt.hist);\n",
    "g.map_offdiag(plt.scatter);\n",
    "g.savefig('vis/kl_div_optim2/hyperparam_maps/gaussian_noise_rand.pdf');\n",
    "\n",
    "# plotting loss function evolution\n",
    "fig = plt.figure(figsize=[16,9]); plt.step(range(len(rand_res_df['loss'])), rand_res_df['loss'], where='mid'); \n",
    "plt.title('Loss over rounds (Random Search)'); plt.xlabel('Round'); plt.ylabel('Loss'); \n",
    "fig.savefig('vis/kl_div_optim2/loss_over_rounds/gaussian_noise_rand.pdf');\n",
    "\n",
    "# tpe #\n",
    "\n",
    "# results data frame\n",
    "tpe_res_df = exp_dict_to_df_hp(experiment_dict['gaussian_noise']['tpe'])\n",
    "\n",
    "# plotting hyperparameter importance\n",
    "g = sns.PairGrid(tpe_res_df, hue='loss');\n",
    "g.map_diag(plt.hist);\n",
    "g.map_offdiag(plt.scatter);\n",
    "g.savefig('vis/kl_div_optim2/hyperparam_maps/gaussian_noise_tpe.pdf');\n",
    "\n",
    "# plotting loss function evolution\n",
    "fig = plt.figure(figsize=[16,9]); plt.step(range(len(tpe_res_df['loss'])), tpe_res_df['loss'], where='mid'); \n",
    "plt.title('Loss over rounds (TPE)'); plt.xlabel('Round'); plt.ylabel('Loss'); \n",
    "fig.savefig('vis/kl_div_optim2/loss_over_rounds/gaussian_noise_tpe.pdf');\n",
    "\n",
    "# gaussian processes #\n",
    "\n",
    "# results data frame\n",
    "gp_res_df = exp_dict_to_df_bo(experiment_dict['gaussian_noise']['gp'])\n",
    "\n",
    "# plotting hyperparameter importance\n",
    "g = sns.PairGrid(gp_res_df, hue='loss');\n",
    "g.map_diag(plt.hist);\n",
    "g.map_offdiag(plt.scatter);\n",
    "g.savefig('vis/kl_div_optim2/hyperparam_maps/gaussian_noise_gp.pdf');\n",
    "\n",
    "# plotting loss function evolution\n",
    "fig = plt.figure(figsize=[16,9]); plt.step(range(len(gp_res_df['loss'])), -gp_res_df['loss'], where='mid'); \n",
    "plt.title('Loss over rounds (GP)'); plt.xlabel('Round'); plt.ylabel('Loss'); \n",
    "fig.savefig('vis/kl_div_optim2/loss_over_rounds/gaussian_noise_gp.pdf');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# showing best embedded spaces #\n",
    "print \"Random Search best:\", [i for i,e in enumerate(experiment_dict['gaussian_noise']['rand']) if e['loss'] == np.min([e['loss'] for e in experiment_dict['gaussian_noise']['rand']])]\n",
    "print \"TPE best:\", [i for i,e in enumerate(experiment_dict['gaussian_noise']['tpe']) if e['loss'] == np.min([e['loss'] for e in experiment_dict['gaussian_noise']['tpe']])]\n",
    "print \"GP best:\", [i for i,e in enumerate(experiment_dict['gaussian_noise']['gp']['values']) if e == np.max(experiment_dict['gaussian_noise']['gp']['values'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Two gaussian distributions with different densities\n",
    "\n",
    "Two gaussian distributions centered on the origin, but with different standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading the data\n",
    "test_df = pd.read_csv('data/final/topology.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# removing target variable\n",
    "test_target = test_df['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# random search #\n",
    "\n",
    "# initializing wrapper - hyperopt\n",
    "opt_task_hp = hp_wrapper(test_df, save_path='vis/kl_div_optim2/all_plots/topology_rand')\n",
    "\n",
    "# trials object stores the evaluations\n",
    "trials_rand = Trials()\n",
    "\n",
    "# using the fmin function from hyperopt\n",
    "best = fmin(fn=opt_task_hp.target, algo=rand.suggest, space=space_hp, max_evals=50, trials=trials_rand)\n",
    "\n",
    "# storing the results\n",
    "experiment_dict['topology']['rand'] = [trials_rand.trials[i]['result'] for i in range(len(trials_rand.trials))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TPE #\n",
    "\n",
    "# initializing wrapper - hyperopt\n",
    "opt_task_hp = hp_wrapper(test_df, save_path='vis/kl_div_optim2/all_plots/topology_tpe')\n",
    "\n",
    "# trials object stores the evaluations\n",
    "trials_tpe = Trials()\n",
    "\n",
    "# using the fmin function from hyperopt\n",
    "best = fmin(fn=opt_task_hp.target, algo=tpe.suggest, space=space_hp, max_evals=50, trials=trials_tpe)\n",
    "\n",
    "# storing the results\n",
    "experiment_dict['topology']['tpe'] = [trials_tpe.trials[i]['result'] for i in range(len(trials_tpe.trials))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gaussian processes #\n",
    "\n",
    "# initializing wrapper - BayesianOptimazion\n",
    "opt_task_bo = bo_wrapper(test_df, save_path='vis/kl_div_optim2/all_plots/topology_gp')\n",
    "\n",
    "# defining optimization object\n",
    "bo = BayesianOptimization(opt_task_bo.target, space_bo, verbose=0)\n",
    "\n",
    "# optimizing\n",
    "bo.maximize(init_points=10, n_iter=40, acq='ucb', kappa=10)\n",
    "\n",
    "# writing results\n",
    "experiment_dict['topology']['gp'] = bo.res['all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# computing figures of the optimization process #\n",
    "\n",
    "# function to transform the experiment dict into a data frame - hyperopt #\n",
    "def exp_dict_to_df_hp(exp_dict):\n",
    "\n",
    "    # dictionary to story intermediate values\n",
    "    temp_dict = dict()\n",
    "\n",
    "    # for each experiment\n",
    "    for dict_entry in exp_dict:\n",
    "\n",
    "        # add parameters to dict\n",
    "        for key in dict_entry['parameters'].keys():\n",
    "            try:\n",
    "                temp_dict[key].append(dict_entry['parameters'][key])\n",
    "            except KeyError:\n",
    "                temp_dict[key] = [dict_entry['parameters'][key]]\n",
    "\n",
    "        # add loss to dict\n",
    "        try:\n",
    "            temp_dict['loss'].append(dict_entry['loss'])\n",
    "        except KeyError:\n",
    "            temp_dict['loss'] = [dict_entry['loss']]\n",
    "    \n",
    "    # return a data frame\n",
    "    return pd.DataFrame(temp_dict)\n",
    "\n",
    "# function to transform the experiment dict into a data frame - bayesian optim #\n",
    "def exp_dict_to_df_bo(exp_dict):\n",
    "\n",
    "    # dictionary to story intermediate values\n",
    "    temp_dict = dict()\n",
    "\n",
    "    # for each experiment\n",
    "    for dict_entry in exp_dict['params']:\n",
    "\n",
    "        # add parameters to dict\n",
    "        for key in dict_entry.keys():\n",
    "            try:\n",
    "                temp_dict[key].append(dict_entry[key])\n",
    "            except KeyError:\n",
    "                temp_dict[key] = [dict_entry[key]]\n",
    "    \n",
    "    # adding loss\n",
    "    temp_dict['loss'] = exp_dict['values']\n",
    "    \n",
    "    # correcting whitening_flag\n",
    "    temp_dict['whitening_flag'] = [False if e < 0.50 else True for e in temp_dict['whitening_flag']]\n",
    "    \n",
    "    # return a data frame\n",
    "    return pd.DataFrame(temp_dict)\n",
    "\n",
    "# random search #\n",
    "\n",
    "# results data frame\n",
    "rand_res_df = exp_dict_to_df_hp(experiment_dict['topology']['rand'])\n",
    "\n",
    "# plotting hyperparameter importance\n",
    "g = sns.PairGrid(rand_res_df, hue='loss');\n",
    "g.map_diag(plt.hist);\n",
    "g.map_offdiag(plt.scatter);\n",
    "g.savefig('vis/kl_div_optim2/hyperparam_maps/topology_rand.pdf');\n",
    "\n",
    "# plotting loss function evolution\n",
    "fig = plt.figure(figsize=[16,9]); plt.step(range(len(rand_res_df['loss'])), rand_res_df['loss'], where='mid'); \n",
    "plt.title('Loss over rounds (Random Search)'); plt.xlabel('Round'); plt.ylabel('Loss'); \n",
    "fig.savefig('vis/kl_div_optim2/loss_over_rounds/topology_rand.pdf');\n",
    "\n",
    "# tpe #\n",
    "\n",
    "# results data frame\n",
    "tpe_res_df = exp_dict_to_df_hp(experiment_dict['topology']['tpe'])\n",
    "\n",
    "# plotting hyperparameter importance\n",
    "g = sns.PairGrid(tpe_res_df, hue='loss');\n",
    "g.map_diag(plt.hist);\n",
    "g.map_offdiag(plt.scatter);\n",
    "g.savefig('vis/kl_div_optim2/hyperparam_maps/topology_tpe.pdf');\n",
    "\n",
    "# plotting loss function evolution\n",
    "fig = plt.figure(figsize=[16,9]); plt.step(range(len(tpe_res_df['loss'])), tpe_res_df['loss'], where='mid'); \n",
    "plt.title('Loss over rounds (TPE)'); plt.xlabel('Round'); plt.ylabel('Loss'); \n",
    "fig.savefig('vis/kl_div_optim2/loss_over_rounds/topology_tpe.pdf');\n",
    "\n",
    "# gaussian processes #\n",
    "\n",
    "# results data frame\n",
    "gp_res_df = exp_dict_to_df_bo(experiment_dict['topology']['gp'])\n",
    "\n",
    "# plotting hyperparameter importance\n",
    "g = sns.PairGrid(gp_res_df, hue='loss');\n",
    "g.map_diag(plt.hist);\n",
    "g.map_offdiag(plt.scatter);\n",
    "g.savefig('vis/kl_div_optim2/hyperparam_maps/topology_gp.pdf');\n",
    "\n",
    "# plotting loss function evolution\n",
    "fig = plt.figure(figsize=[16,9]); plt.step(range(len(gp_res_df['loss'])), -gp_res_df['loss'], where='mid'); \n",
    "plt.title('Loss over rounds (GP)'); plt.xlabel('Round'); plt.ylabel('Loss'); \n",
    "fig.savefig('vis/kl_div_optim2/loss_over_rounds/topology_gp.pdf');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# showing best embedded spaces #\n",
    "print \"Random Search best:\", [i for i,e in enumerate(experiment_dict['topology']['rand']) if e['loss'] == np.min([e['loss'] for e in experiment_dict['topology']['rand']])]\n",
    "print \"TPE best:\", [i for i,e in enumerate(experiment_dict['topology']['tpe']) if e['loss'] == np.min([e['loss'] for e in experiment_dict['topology']['tpe']])]\n",
    "print \"GP best:\", [i for i,e in enumerate(experiment_dict['topology']['gp']['values']) if e == np.max(experiment_dict['topology']['gp']['values'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 COIL-20\n",
    "\n",
    "Images of rotated objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading the data\n",
    "test_df = pd.read_csv('data/final/coil-20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# removing target variable\n",
    "test_target = test_df['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# random search #\n",
    "\n",
    "# initializing wrapper - hyperopt\n",
    "opt_task_hp = hp_wrapper(test_df, save_path='vis/kl_div_optim2/all_plots/coil_20_rand')\n",
    "\n",
    "# trials object stores the evaluations\n",
    "trials_rand = Trials()\n",
    "\n",
    "# using the fmin function from hyperopt\n",
    "best = fmin(fn=opt_task_hp.target, algo=rand.suggest, space=space_hp, max_evals=50, trials=trials_rand)\n",
    "\n",
    "# storing the results\n",
    "experiment_dict['coil_20']['rand'] = [trials_rand.trials[i]['result'] for i in range(len(trials_rand.trials))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TPE #\n",
    "\n",
    "# initializing wrapper - hyperopt\n",
    "opt_task_hp = hp_wrapper(test_df, save_path='vis/kl_div_optim2/all_plots/coil_20_tpe')\n",
    "\n",
    "# trials object stores the evaluations\n",
    "trials_tpe = Trials()\n",
    "\n",
    "# using the fmin function from hyperopt\n",
    "best = fmin(fn=opt_task_hp.target, algo=tpe.suggest, space=space_hp, max_evals=50, trials=trials_tpe)\n",
    "\n",
    "# storing the results\n",
    "experiment_dict['coil_20']['tpe'] = [trials_tpe.trials[i]['result'] for i in range(len(trials_tpe.trials))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gaussian processes #\n",
    "\n",
    "# initializing wrapper - BayesianOptimazion\n",
    "opt_task_bo = bo_wrapper(test_df, save_path='vis/kl_div_optim2/all_plots/coil_20_gp')\n",
    "\n",
    "# defining optimization object\n",
    "bo = BayesianOptimization(opt_task_bo.target, space_bo, verbose=0)\n",
    "\n",
    "# optimizing\n",
    "bo.maximize(init_points=10, n_iter=40, acq='ucb', kappa=10)\n",
    "\n",
    "# writing results\n",
    "experiment_dict['coil_20']['gp'] = bo.res['all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# computing figures of the optimization process #\n",
    "\n",
    "# function to transform the experiment dict into a data frame - hyperopt #\n",
    "def exp_dict_to_df_hp(exp_dict):\n",
    "\n",
    "    # dictionary to story intermediate values\n",
    "    temp_dict = dict()\n",
    "\n",
    "    # for each experiment\n",
    "    for dict_entry in exp_dict:\n",
    "\n",
    "        # add parameters to dict\n",
    "        for key in dict_entry['parameters'].keys():\n",
    "            try:\n",
    "                temp_dict[key].append(dict_entry['parameters'][key])\n",
    "            except KeyError:\n",
    "                temp_dict[key] = [dict_entry['parameters'][key]]\n",
    "\n",
    "        # add loss to dict\n",
    "        try:\n",
    "            temp_dict['loss'].append(dict_entry['loss'])\n",
    "        except KeyError:\n",
    "            temp_dict['loss'] = [dict_entry['loss']]\n",
    "    \n",
    "    # return a data frame\n",
    "    return pd.DataFrame(temp_dict)\n",
    "\n",
    "# function to transform the experiment dict into a data frame - bayesian optim #\n",
    "def exp_dict_to_df_bo(exp_dict):\n",
    "\n",
    "    # dictionary to story intermediate values\n",
    "    temp_dict = dict()\n",
    "\n",
    "    # for each experiment\n",
    "    for dict_entry in exp_dict['params']:\n",
    "\n",
    "        # add parameters to dict\n",
    "        for key in dict_entry.keys():\n",
    "            try:\n",
    "                temp_dict[key].append(dict_entry[key])\n",
    "            except KeyError:\n",
    "                temp_dict[key] = [dict_entry[key]]\n",
    "    \n",
    "    # adding loss\n",
    "    temp_dict['loss'] = exp_dict['values']\n",
    "    \n",
    "    # correcting whitening_flag\n",
    "    temp_dict['whitening_flag'] = [False if e < 0.50 else True for e in temp_dict['whitening_flag']]\n",
    "    \n",
    "    # return a data frame\n",
    "    return pd.DataFrame(temp_dict)\n",
    "\n",
    "# random search #\n",
    "\n",
    "# results data frame\n",
    "rand_res_df = exp_dict_to_df_hp(experiment_dict['coil_20']['rand'])\n",
    "\n",
    "# plotting hyperparameter importance\n",
    "g = sns.PairGrid(rand_res_df, hue='loss');\n",
    "g.map_diag(plt.hist);\n",
    "g.map_offdiag(plt.scatter);\n",
    "g.savefig('vis/kl_div_optim2/hyperparam_maps/coil_20_rand.pdf');\n",
    "\n",
    "# plotting loss function evolution\n",
    "fig = plt.figure(figsize=[16,9]); plt.step(range(len(rand_res_df['loss'])), rand_res_df['loss'], where='mid'); \n",
    "plt.title('Loss over rounds (Random Search)'); plt.xlabel('Round'); plt.ylabel('Loss'); \n",
    "fig.savefig('vis/kl_div_optim2/loss_over_rounds/coil_20_rand.pdf');\n",
    "\n",
    "# tpe #\n",
    "\n",
    "# results data frame\n",
    "tpe_res_df = exp_dict_to_df_hp(experiment_dict['coil_20']['tpe'])\n",
    "\n",
    "# plotting hyperparameter importance\n",
    "g = sns.PairGrid(tpe_res_df, hue='loss');\n",
    "g.map_diag(plt.hist);\n",
    "g.map_offdiag(plt.scatter);\n",
    "g.savefig('vis/kl_div_optim2/hyperparam_maps/coil_20_tpe.pdf');\n",
    "\n",
    "# plotting loss function evolution\n",
    "fig = plt.figure(figsize=[16,9]); plt.step(range(len(tpe_res_df['loss'])), tpe_res_df['loss'], where='mid'); \n",
    "plt.title('Loss over rounds (TPE)'); plt.xlabel('Round'); plt.ylabel('Loss'); \n",
    "fig.savefig('vis/kl_div_optim2/loss_over_rounds/coil_20_tpe.pdf');\n",
    "\n",
    "# gaussian processes #\n",
    "\n",
    "# results data frame\n",
    "gp_res_df = exp_dict_to_df_bo(experiment_dict['coil_20']['gp'])\n",
    "\n",
    "# plotting hyperparameter importance\n",
    "g = sns.PairGrid(gp_res_df, hue='loss');\n",
    "g.map_diag(plt.hist);\n",
    "g.map_offdiag(plt.scatter);\n",
    "g.savefig('vis/kl_div_optim2/hyperparam_maps/coil_20_gp.pdf');\n",
    "\n",
    "# plotting loss function evolution\n",
    "fig = plt.figure(figsize=[16,9]); plt.step(range(len(gp_res_df['loss'])), -gp_res_df['loss'], where='mid'); \n",
    "plt.title('Loss over rounds (GP)'); plt.xlabel('Round'); plt.ylabel('Loss'); \n",
    "fig.savefig('vis/kl_div_optim2/loss_over_rounds/coil_20_gp.pdf');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# showing best embedded spaces #\n",
    "print \"Random Search best:\", [i for i,e in enumerate(experiment_dict['coil_20']['rand']) if e['loss'] == np.min([e['loss'] for e in experiment_dict['coil_20']['rand']])]\n",
    "print \"TPE best:\", [i for i,e in enumerate(experiment_dict['coil_20']['tpe']) if e['loss'] == np.min([e['loss'] for e in experiment_dict['coil_20']['tpe']])]\n",
    "print \"GP best:\", [i for i,e in enumerate(experiment_dict['coil_20']['gp']['values']) if e == np.max(experiment_dict['coil_20']['gp']['values'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Olivetti faces\n",
    "\n",
    "Pictures of different people with small variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reading the data\n",
    "test_df = pd.read_csv('data/final/olivetti-faces.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# removing target variable\n",
    "test_target = test_df['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# random search #\n",
    "\n",
    "# initializing wrapper - hyperopt\n",
    "opt_task_hp = hp_wrapper(test_df, save_path='vis/kl_div_optim2/all_plots/olivetti_rand')\n",
    "\n",
    "# trials object stores the evaluations\n",
    "trials_rand = Trials()\n",
    "\n",
    "# using the fmin function from hyperopt\n",
    "best = fmin(fn=opt_task_hp.target, algo=rand.suggest, space=space_hp, max_evals=50, trials=trials_rand)\n",
    "\n",
    "# storing the results\n",
    "experiment_dict['olivetti']['rand'] = [trials_rand.trials[i]['result'] for i in range(len(trials_rand.trials))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TPE #\n",
    "\n",
    "# initializing wrapper - hyperopt\n",
    "opt_task_hp = hp_wrapper(test_df, save_path='vis/kl_div_optim2/all_plots/olivetti_tpe')\n",
    "\n",
    "# trials object stores the evaluations\n",
    "trials_tpe = Trials()\n",
    "\n",
    "# using the fmin function from hyperopt\n",
    "best = fmin(fn=opt_task_hp.target, algo=tpe.suggest, space=space_hp, max_evals=50, trials=trials_tpe)\n",
    "\n",
    "# storing the results\n",
    "experiment_dict['olivetti']['tpe'] = [trials_tpe.trials[i]['result'] for i in range(len(trials_tpe.trials))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gaussian processes #\n",
    "\n",
    "# initializing wrapper - BayesianOptimazion\n",
    "opt_task_bo = bo_wrapper(test_df, save_path='vis/kl_div_optim2/all_plots/olivetti_gp')\n",
    "\n",
    "# defining optimization object\n",
    "bo = BayesianOptimization(opt_task_bo.target, space_bo, verbose=0)\n",
    "\n",
    "# optimizing\n",
    "bo.maximize(init_points=10, n_iter=40, acq='ucb', kappa=10)\n",
    "\n",
    "# writing results\n",
    "experiment_dict['olivetti']['gp'] = bo.res['all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# computing figures of the optimization process #\n",
    "\n",
    "# function to transform the experiment dict into a data frame - hyperopt #\n",
    "def exp_dict_to_df_hp(exp_dict):\n",
    "\n",
    "    # dictionary to story intermediate values\n",
    "    temp_dict = dict()\n",
    "\n",
    "    # for each experiment\n",
    "    for dict_entry in exp_dict:\n",
    "\n",
    "        # add parameters to dict\n",
    "        for key in dict_entry['parameters'].keys():\n",
    "            try:\n",
    "                temp_dict[key].append(dict_entry['parameters'][key])\n",
    "            except KeyError:\n",
    "                temp_dict[key] = [dict_entry['parameters'][key]]\n",
    "\n",
    "        # add loss to dict\n",
    "        try:\n",
    "            temp_dict['loss'].append(dict_entry['loss'])\n",
    "        except KeyError:\n",
    "            temp_dict['loss'] = [dict_entry['loss']]\n",
    "    \n",
    "    # return a data frame\n",
    "    return pd.DataFrame(temp_dict)\n",
    "\n",
    "# function to transform the experiment dict into a data frame - bayesian optim #\n",
    "def exp_dict_to_df_bo(exp_dict):\n",
    "\n",
    "    # dictionary to story intermediate values\n",
    "    temp_dict = dict()\n",
    "\n",
    "    # for each experiment\n",
    "    for dict_entry in exp_dict['params']:\n",
    "\n",
    "        # add parameters to dict\n",
    "        for key in dict_entry.keys():\n",
    "            try:\n",
    "                temp_dict[key].append(dict_entry[key])\n",
    "            except KeyError:\n",
    "                temp_dict[key] = [dict_entry[key]]\n",
    "    \n",
    "    # adding loss\n",
    "    temp_dict['loss'] = exp_dict['values']\n",
    "    \n",
    "    # correcting whitening_flag\n",
    "    temp_dict['whitening_flag'] = [False if e < 0.50 else True for e in temp_dict['whitening_flag']]\n",
    "    \n",
    "    # return a data frame\n",
    "    return pd.DataFrame(temp_dict)\n",
    "\n",
    "# random search #\n",
    "\n",
    "# results data frame\n",
    "rand_res_df = exp_dict_to_df_hp(experiment_dict['olivetti']['rand'])\n",
    "\n",
    "# plotting hyperparameter importance\n",
    "g = sns.PairGrid(rand_res_df, hue='loss');\n",
    "g.map_diag(plt.hist);\n",
    "g.map_offdiag(plt.scatter);\n",
    "g.savefig('vis/kl_div_optim2/hyperparam_maps/olivetti_rand.pdf');\n",
    "\n",
    "# plotting loss function evolution\n",
    "fig = plt.figure(figsize=[16,9]); plt.step(range(len(rand_res_df['loss'])), rand_res_df['loss'], where='mid'); \n",
    "plt.title('Loss over rounds (Random Search)'); plt.xlabel('Round'); plt.ylabel('Loss'); \n",
    "fig.savefig('vis/kl_div_optim2/loss_over_rounds/olivetti_rand.pdf');\n",
    "\n",
    "# tpe #\n",
    "\n",
    "# results data frame\n",
    "tpe_res_df = exp_dict_to_df_hp(experiment_dict['olivetti']['tpe'])\n",
    "\n",
    "# plotting hyperparameter importance\n",
    "g = sns.PairGrid(tpe_res_df, hue='loss');\n",
    "g.map_diag(plt.hist);\n",
    "g.map_offdiag(plt.scatter);\n",
    "g.savefig('vis/kl_div_optim2/hyperparam_maps/olivetti_tpe.pdf');\n",
    "\n",
    "# plotting loss function evolution\n",
    "fig = plt.figure(figsize=[16,9]); plt.step(range(len(tpe_res_df['loss'])), tpe_res_df['loss'], where='mid'); \n",
    "plt.title('Loss over rounds (TPE)'); plt.xlabel('Round'); plt.ylabel('Loss'); \n",
    "fig.savefig('vis/kl_div_optim2/loss_over_rounds/olivetti_tpe.pdf');\n",
    "\n",
    "# gaussian processes #\n",
    "\n",
    "# results data frame\n",
    "gp_res_df = exp_dict_to_df_bo(experiment_dict['olivetti']['gp'])\n",
    "\n",
    "# plotting hyperparameter importance\n",
    "g = sns.PairGrid(gp_res_df, hue='loss');\n",
    "g.map_diag(plt.hist);\n",
    "g.map_offdiag(plt.scatter);\n",
    "g.savefig('vis/kl_div_optim2/hyperparam_maps/olivetti_gp.pdf');\n",
    "\n",
    "# plotting loss function evolution\n",
    "fig = plt.figure(figsize=[16,9]); plt.step(range(len(gp_res_df['loss'])), -gp_res_df['loss'], where='mid'); \n",
    "plt.title('Loss over rounds (GP)'); plt.xlabel('Round'); plt.ylabel('Loss'); \n",
    "fig.savefig('vis/kl_div_optim2/loss_over_rounds/olivetti_gp.pdf');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# showing best embedded spaces #\n",
    "print \"Random Search best:\", [i for i,e in enumerate(experiment_dict['olivetti']['rand']) if e['loss'] == np.min([e['loss'] for e in experiment_dict['olivetti']['rand']])]\n",
    "print \"TPE best:\", [i for i,e in enumerate(experiment_dict['olivetti']['tpe']) if e['loss'] == np.min([e['loss'] for e in experiment_dict['olivetti']['tpe']])]\n",
    "print \"GP best:\", [i for i,e in enumerate(experiment_dict['olivetti']['gp']['values']) if e == np.max(experiment_dict['olivetti']['gp']['values'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Final Data & Plots\n",
    "\n",
    "Let us wrap the data up for publication, with includes the generation of new plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# computing figures of the optimization process #\n",
    "\n",
    "# function to transform the experiment dict into a data frame - hyperopt #\n",
    "def exp_dict_to_df_hp(exp_dict):\n",
    "\n",
    "    # dictionary to story intermediate values\n",
    "    temp_dict = dict()\n",
    "\n",
    "    # for each experiment\n",
    "    for dict_entry in exp_dict:\n",
    "\n",
    "        # add parameters to dict\n",
    "        for key in dict_entry['parameters'].keys():\n",
    "            try:\n",
    "                temp_dict[key].append(dict_entry['parameters'][key])\n",
    "            except KeyError:\n",
    "                temp_dict[key] = [dict_entry['parameters'][key]]\n",
    "\n",
    "        # add loss to dict\n",
    "        try:\n",
    "            temp_dict['loss'].append(dict_entry['loss'])\n",
    "        except KeyError:\n",
    "            temp_dict['loss'] = [dict_entry['loss']]\n",
    "    \n",
    "    # return a data frame\n",
    "    return pd.DataFrame(temp_dict)\n",
    "\n",
    "# function to transform the experiment dict into a data frame - bayesian optim #\n",
    "def exp_dict_to_df_bo(exp_dict):\n",
    "\n",
    "    # dictionary to story intermediate values\n",
    "    temp_dict = dict()\n",
    "\n",
    "    # for each experiment\n",
    "    for dict_entry in exp_dict['params']:\n",
    "\n",
    "        # add parameters to dict\n",
    "        for key in dict_entry.keys():\n",
    "            try:\n",
    "                temp_dict[key].append(dict_entry[key])\n",
    "            except KeyError:\n",
    "                temp_dict[key] = [dict_entry[key]]\n",
    "    \n",
    "    # adding loss\n",
    "    temp_dict['loss'] = -1*np.array(exp_dict['values'])\n",
    "    \n",
    "    # correcting whitening_flag\n",
    "    temp_dict['whitening_flag'] = [False if e < 0.50 else True for e in temp_dict['whitening_flag']]\n",
    "    \n",
    "    # return a data frame\n",
    "    return pd.DataFrame(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# opening figure\n",
    "fig = plt.figure(figsize=[16,20], dpi=300)\n",
    "\n",
    "# count variable\n",
    "count = 0\n",
    "\n",
    "# x-axis limits\n",
    "x_limits = {'well_sep': [0.4,2.2], 'well_sep_noise': [0.4,1.6], 'gaussian_noise': [1.4,2.1],\n",
    "            'topology': [1.2,1.9], 'coil_20': [0.5, 3.0], 'olivetti': [0.5, 1.2]}\n",
    "y_limits = {'well_sep': [0.0, 12.0], 'well_sep_noise': [0.0, 14.0], 'gaussian_noise': [0.0,16.0],\n",
    "            'topology': [0.0,10.0], 'coil_20': [0.0, 16.0], 'olivetti': [0.0, 9.0]}\n",
    "# realizing joined graphs\n",
    "for dataset in ['well_sep', 'well_sep_noise', 'gaussian_noise', 'topology', 'coil_20', 'olivetti']:\n",
    "        \n",
    "    for optim in ['rand', 'tpe', 'gp']:\n",
    "        \n",
    "        # choosing right function \n",
    "        if optim == 'gp':\n",
    "\n",
    "            # results data frame\n",
    "            res_df = exp_dict_to_df_bo(experiment_dict[dataset][optim])            \n",
    "            \n",
    "        # if not gp, use hp data treting function\n",
    "        else:\n",
    "            \n",
    "            # results data frame\n",
    "            res_df = exp_dict_to_df_hp(experiment_dict[dataset][optim])\n",
    "\n",
    "        # plotting loss function evolution\n",
    "        plt.subplot(6, 3, count + 1); sns.distplot(res_df.loc[res_df['loss'] < 100,'loss'], label=optim, bins=30, kde=False); plt.tight_layout()\n",
    "        plt.title(u'Resultados de otimizao [{}-{}]'.format(dataset, optim)); plt.xlabel('Resultado'); plt.ylabel(u'Frequncia'); \n",
    "        plt.xlim(x_limits[dataset]); plt.ylim(y_limits[dataset])\n",
    "\n",
    "        # adding to count\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig.savefig('vis/kl_div_optim2/loss_over_rounds/kl-div-loss-histo.pdf');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max, min, mean per optimization round table\n",
    "summary_df = pd.DataFrame()\n",
    "\n",
    "# realizing joined graphs\n",
    "for dataset in ['well_sep', 'well_sep_noise', 'gaussian_noise', 'topology', 'coil_20', 'olivetti']:\n",
    "        \n",
    "    for optim in ['rand', 'tpe', 'gp']:\n",
    "        \n",
    "        # choosing right function \n",
    "        if optim == 'gp':\n",
    "\n",
    "            # results data frame\n",
    "            res_df = exp_dict_to_df_bo(experiment_dict[dataset][optim])            \n",
    "            \n",
    "        # if not gp, use hp data treting function\n",
    "        else:\n",
    "            \n",
    "            # results data frame\n",
    "            res_df = exp_dict_to_df_hp(experiment_dict[dataset][optim])\n",
    "\n",
    "        # temporary df\n",
    "        temp_df = pd.DataFrame()\n",
    "        temp_df.loc[:,'dataset'] = [dataset]\n",
    "        temp_df.loc[:,'optim'] = [optim]\n",
    "        temp_df.loc[:,'mean_loss'] = [res_df['loss'].mean()]\n",
    "        temp_df.loc[:,'max_loss'] = [res_df['loss'].max()]\n",
    "        temp_df.loc[:,'min_loss'] = [res_df['loss'].min()]\n",
    "        \n",
    "        # accumulating\n",
    "        summary_df = pd.concat([summary_df, temp_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary_df.drop('dataset', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# finding the distributions of parameters for the best experiments #\n",
    "\n",
    "# opening figure\n",
    "f = plt.figure(figsize=[9,9], dpi=300)\n",
    "\n",
    "# accummulation dict\n",
    "top5_dict = {}\n",
    "\n",
    "# realizing joined graphs\n",
    "for i, dataset in enumerate(['well_sep', 'well_sep_noise', 'gaussian_noise', 'topology', 'coil_20', 'olivetti']):\n",
    "    \n",
    "    # df for storing all the best experiments\n",
    "    dataset_res_df = pd.DataFrame()\n",
    "        \n",
    "    for optim in ['rand', 'tpe', 'gp']:\n",
    "        \n",
    "        # choosing right function \n",
    "        if optim == 'gp':\n",
    "\n",
    "            # results data frame\n",
    "            res_df = exp_dict_to_df_bo(experiment_dict[dataset][optim]); res_df.loc[:,'optim'] = optim            \n",
    "            \n",
    "        # if not gp, use hp data treting function\n",
    "        else:\n",
    "            \n",
    "            # results data frame\n",
    "            res_df = exp_dict_to_df_hp(experiment_dict[dataset][optim]); res_df.loc[:,'optim'] = optim \n",
    "        \n",
    "        # updating \n",
    "        dataset_res_df = pd.concat([dataset_res_df, res_df])\n",
    "    \n",
    "    # filtering top 5 trials\n",
    "    top5 = dataset_res_df.sort_values('loss', ascending=True).head(5)\n",
    "    top5.loc[:,'whitening_flag'] = top5.loc[:,'whitening_flag'].astype(str)\n",
    "    \n",
    "    # accumulating\n",
    "    top5_dict[dataset] = top5.loc[:, ['loss','angle','early_exaggeration','learning_rate','n_iter','pca_dims','perplexity','whitening_flag','optim']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top5_dict['olivetti'].to_latex('results_temp.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# funo para salvar dicionrio de experimentos\n",
    "import pickle\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, 0)\n",
    "        \n",
    "# salvando dicionrio de resultados\n",
    "save_obj(experiment_dict, 'trials/kl-div2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_obj(name ):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
